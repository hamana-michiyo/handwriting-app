"""
æ”¹è‰¯ç‰ˆOCR: page_split.pyã®é«˜ç²¾åº¦æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯çµ±åˆç‰ˆ
"""
import cv2
import numpy as np
import pytesseract
from typing import Dict, List, Tuple, Optional
import logging
import os
from pathlib import Path
from dotenv import load_dotenv
import torch
from PIL import Image, ImageOps
import torchvision.transforms as transforms

# .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

# Gemini ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from .gemini_client import GeminiCharacterRecognizer
    GEMINI_AVAILABLE = True
except ImportError:
    try:
        from gemini_client import GeminiCharacterRecognizer
        GEMINI_AVAILABLE = True
    except ImportError:
        GEMINI_AVAILABLE = False
        logger.warning("Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

logger = logging.getLogger(__name__)

class SimpleCNN(torch.nn.Module):
    """MNIST+æ‰‹æ›¸ãæ•°å­—èªè­˜ç”¨ã®CNNãƒ¢ãƒ‡ãƒ«"""
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = torch.nn.Conv2d(1, 16, 3, 1)
        self.fc1 = torch.nn.Linear(26*26*16, 128)
        self.fc2 = torch.nn.Linear(128, 11)  # 0ã€œ10

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = x.view(-1, 26*26*16)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class ImprovedOCRProcessor:
    """æ”¹è‰¯ç‰ˆOCRå‡¦ç†ã‚¯ãƒ©ã‚¹ï¼ˆpage_split.pyãƒ­ã‚¸ãƒƒã‚¯çµ±åˆç‰ˆï¼‰"""
    
    def __init__(self, use_gemini=True, debug_dir="debug"):
        self.tombo_size_range = (8, 30)
        self.tombo_area_range = (50, 900)
        self.corrected_image = None
        self.corrected_width = None
        self.corrected_height = None
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        self.debug_dir = Path(debug_dir)
        self.debug_dir.mkdir(parents=True, exist_ok=True)
        
        # Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.gemini_client = None
        self.use_gemini = use_gemini and GEMINI_AVAILABLE
        
        if self.use_gemini:
            try:
                self.gemini_client = GeminiCharacterRecognizer()
                logger.info("Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•—: {e}")
                self.use_gemini = False
        
        # PyTorchãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.pytorch_model = None
        self.use_pytorch = False
        try:
            self.pytorch_model = SimpleCNN()
            model_path = "/workspace/data/digit_model.pt"
            if os.path.exists(model_path):
                self.pytorch_model.load_state_dict(torch.load(model_path, map_location='cpu'))
                self.pytorch_model.eval()
                self.use_pytorch = True
                logger.info("PyTorchæ•°å­—èªè­˜ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æˆåŠŸ")
            else:
                logger.warning(f"PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        except Exception as e:
            logger.warning(f"PyTorchãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
            self.use_pytorch = False
    
    def find_page_corners(self, gray: np.ndarray) -> Optional[np.ndarray]:
        """page_split.pyã®ãƒšãƒ¼ã‚¸æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯"""
        try:
            blur = cv2.GaussianBlur(gray, (5, 5), 0)
            _, bw = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            
            # èƒŒæ™¯ãŒæ¿ƒã„å ´åˆã®åè»¢
            if bw.mean() < 127:
                bw = cv2.bitwise_not(bw)
            
            contours, _ = cv2.findContours(bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if not contours:
                return None
            
            page = max(contours, key=cv2.contourArea)
            peri = cv2.arcLength(page, True)
            approx = cv2.approxPolyDP(page, 0.02 * peri, True)
            
            if len(approx) != 4:
                return None
            
            return approx.reshape(4, 2).astype("float32")
        except Exception as e:
            logger.warning(f"ãƒšãƒ¼ã‚¸æ¤œå‡ºå¤±æ•—: {e}")
            return None
    
    def order_corners(self, pts: np.ndarray) -> np.ndarray:
        """4ç‚¹ã‚’TL, TR, BR, BLé †ã«ä¸¦ã¹ã‚‹"""
        pts = pts[np.argsort(pts[:, 0])]  # x ã§å·¦2 / å³2
        left, right = pts[:2], pts[2:]
        tl = left[np.argmin(left[:, 1])]
        bl = left[np.argmax(left[:, 1])]
        tr = right[np.argmin(right[:, 1])]
        br = right[np.argmax(right[:, 1])]
        return np.array([tl, tr, br, bl], dtype="float32")
    
    def perspective_correct_advanced(self, img: np.ndarray, corners: np.ndarray, dbg=False) -> np.ndarray:
        """page_split.pyã®é€è¦–å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯"""
        pts = self.order_corners(corners)
        
        wA = np.linalg.norm(pts[2] - pts[3])
        wB = np.linalg.norm(pts[1] - pts[0])
        hA = np.linalg.norm(pts[1] - pts[2])
        hB = np.linalg.norm(pts[0] - pts[3])
        W = int(max(wA, wB))
        H = int(max(hA, hB))
        
        dst = np.array([[0, 0], [W - 1, 0], [W - 1, H - 1], [0, H - 1]], dtype="float32")
        M = cv2.getPerspectiveTransform(pts, dst)
        warped = cv2.warpPerspective(img, M, (W, H))
        
        if dbg:
            dbg_img = img.copy()
            for (x, y) in pts.astype(int):
                cv2.circle(dbg_img, (x, y), 10, (0, 0, 255), -1)
            cv2.imwrite(str(self.debug_dir / "dbg_corners.jpg"), dbg_img)
            cv2.imwrite(str(self.debug_dir / "dbg_warped.jpg"), warped)
        
        return warped
    
    def correct_perspective(self, image: np.ndarray, debug=False) -> np.ndarray:
        """page_split.pyãƒ™ãƒ¼ã‚¹ã®é€è¦–å¤‰æ›"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        
        # ãƒšãƒ¼ã‚¸æ¤œå‡ºã‚’è©¦è¡Œ
        corners = self.find_page_corners(gray)
        
        if corners is not None:
            # é€è¦–å¤‰æ›å®Ÿè¡Œ
            warped = self.perspective_correct_advanced(image, corners, debug)
            logger.info(f"é€è¦–å¤‰æ›é©ç”¨: {warped.shape}")
            return warped
        else:
            # è£œæ­£ã‚¹ã‚­ãƒƒãƒ—
            logger.info("ãƒšãƒ¼ã‚¸æ¤œå‡ºå¤±æ•—ã€è£œæ­£ã‚¹ã‚­ãƒƒãƒ—")
            return image.copy()
    
    def detect_char_cells(self, gray: np.ndarray, debug=False) -> List[Tuple[int, int, int, int]]:
        """page_split.pyã®æ–‡å­—ã‚»ãƒ«æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯"""
        try:
            H, W = gray.shape
            
            # äºŒå€¤åŒ– â†’ ç´°ç·šã‚’å¤ªã‚‰ã›ã¦è¼ªéƒ­ã‚’é–‰ã˜ã‚‹
            th = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 15, 8)
            kernel = np.ones((3, 3), np.uint8)
            th = cv2.dilate(th, kernel, iterations=1)
            
            # è¼ªéƒ­æŠ½å‡º
            cnts, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # ã»ã¼æ­£æ–¹å½¢ãƒ»é¢ç©é—¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿
            cand = []
            for c in cnts:
                area = cv2.contourArea(c)
                if area < (H * W) * 0.002 or area > (H * W) * 0.03:
                    continue
                peri = cv2.arcLength(c, True)
                approx = cv2.approxPolyDP(c, 0.02 * peri, True)
                if len(approx) != 4:
                    continue
                x, y, w, h = cv2.boundingRect(approx)
                if 0.85 < w / h < 1.15:  # æ­£æ–¹å½¢ã£ã½ã„
                    cand.append((x, y, w, h))
            
            if len(cand) < 3:
                raise RuntimeError("èª²é¡Œãƒã‚¹ã‚‰ã—ãå››è§’ãŒ 3 ã¤è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # åˆ—ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚° â†’ å³åˆ—ã‚’é¸æŠ
            xs = np.array([[c[0]] for c in cand], np.float32)
            _, labels, centers = cv2.kmeans(xs, 2, None,
                (cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0),
                10, cv2.KMEANS_PP_CENTERS)
            right_cluster = np.argmax(centers)  # x ãŒå¤§ãã„æ–¹
            right_boxes = [c for c,l in zip(cand, labels.flatten()) if l == right_cluster]
            
            # y ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šâ†’ä¸‹ 3 ã¤ã‚’å–å¾—
            right_boxes = sorted(right_boxes, key=lambda b: b[1])[:3]
            cells = []
            margin = 10
            for x, y, w, h in right_boxes:
                cells.append((x + margin, y + margin, x + w - margin, y + h - margin))
            
            if debug:
                dbg = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
                for (x1,y1,x2,y2) in cells:
                    cv2.rectangle(dbg, (x1,y1), (x2,y2), (0,255,0), 2)
                cv2.imwrite(str(self.debug_dir / "dbg_cells_contour.jpg"), dbg)
            
            if len(cells) != 3:
                raise RuntimeError("èª²é¡Œãƒã‚¹ã‚’ 3 ã¤å–å¾—ã§ãã¾ã›ã‚“")
            
            return cells
        
        except Exception as e:
            logger.warning(f"æ–‡å­—ã‚»ãƒ«æ¤œå‡ºå¤±æ•—: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å›ºå®šæ¯”ç‡
            H, W = gray.shape
            W0 = int(W*0.07); W1 = int(W*0.27)
            cell_h = int(H*0.23); gap = int(H*0.31)
            cells = [(W0, int(H*0.11)+i*gap, W1, int(H*0.11)+i*gap+cell_h) for i in range(3)]
            return cells
    
    def recognize_character_with_gemini(self, image: np.ndarray, char_name: str) -> Dict[str, any]:
        """Gemini APIã‚’ä½¿ç”¨ã—ãŸæ–‡å­—èªè­˜"""
        if not self.use_gemini or self.gemini_client is None:
            return {
                "character": "",
                "confidence": 0.0,
                "method": "gemini_unavailable"
            }
        
        try:
            context = f"æ‰‹æ›¸ãç·´ç¿’ç”¨ç´™ã®æ–‡å­—æ {char_name}ã‹ã‚‰åˆ‡ã‚Šå‡ºã•ã‚ŒãŸæ‰‹æ›¸ãæ–‡å­—"            
            result = self.gemini_client.recognize_japanese_character(image, context)
            
            return {
                "character": result.get("character", ""),
                "confidence": result.get("confidence", 0.0),
                "alternatives": result.get("alternatives", []),
                "reasoning": result.get("reasoning", ""),
                "method": "gemini"
            }
            
        except Exception as e:
            logger.error(f"Geminiæ–‡å­—èªè­˜ã‚¨ãƒ©ãƒ¼ ({char_name}): {e}")
            return {
                "character": "",
                "confidence": 0.0,
                "method": "gemini_error",
                "error": str(e)
            }
    
    def extract_character_regions(self, corrected_image: np.ndarray, debug=False) -> Dict:
        """page_split.pyãƒ™ãƒ¼ã‚¹ã®æ–‡å­—é ˜åŸŸæŠ½å‡º + Geminiæ–‡å­—èªè­˜"""
        gray = cv2.cvtColor(corrected_image, cv2.COLOR_BGR2GRAY) if len(corrected_image.shape) == 3 else corrected_image
        
        # æ–‡å­—ã‚»ãƒ«æ¤œå‡º
        cells = self.detect_char_cells(gray, debug)
        
        character_results = {}
        char_names = ["char_1", "char_2", "char_3"]  # æ±ç”¨å
        
        for i, (x1, y1, x2, y2) in enumerate(cells):
            name = char_names[i]
            region_image = gray[y1:y2, x1:x2]
            
            # è£œåŠ©ç·šé™¤å»ã‚’é©ç”¨
            cleaned_region = self.remove_guidelines(region_image, save_debug=True, debug_name=name)
            
            # ãƒ‡ãƒãƒƒã‚°ä¿å­˜ï¼ˆè£œåŠ©ç·šé™¤å»å‰ï¼‰
            char_file = self.debug_dir / f"improved_char_{name}_original.jpg"
            cv2.imwrite(str(char_file), region_image)
            
            # ãƒ‡ãƒãƒƒã‚°ä¿å­˜ï¼ˆè£œåŠ©ç·šé™¤å»å¾Œï¼‰
            char_file_cleaned = self.debug_dir / f"improved_char_{name}.jpg"
            cv2.imwrite(str(char_file_cleaned), cleaned_region)
            logger.info(f"{name}æ–‡å­—é ˜åŸŸä¿å­˜ï¼ˆè£œåŠ©ç·šé™¤å»æ¸ˆã¿ï¼‰: {char_file_cleaned}")
            
            # Geminiæ–‡å­—èªè­˜ï¼ˆè£œåŠ©ç·šé™¤å»å¾Œã®ç”»åƒã‚’ä½¿ç”¨ï¼‰
            gemini_result = self.recognize_character_with_gemini(cleaned_region, name)
            
            character_results[name] = {
                "image": region_image,  # å…ƒç”»åƒ
                "cleaned_image": cleaned_region,  # è£œåŠ©ç·šé™¤å»å¾Œ
                "bbox": (x1, y1, x2, y2),
                "gemini_recognition": gemini_result
            }
            
            # èªè­˜çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            if gemini_result["character"]:
                confidence = gemini_result["confidence"]
                char = gemini_result["character"]
                logger.info(f"{name} Geminièªè­˜: '{char}' (ä¿¡é ¼åº¦: {confidence:.2f})")
            else:
                logger.info(f"{name} Geminièªè­˜: èªè­˜å¤±æ•—")
        
        return character_results
    
    def remove_guidelines(self, image: np.ndarray, save_debug=False, debug_name="") -> np.ndarray:
        """è£œåŠ©ç·šé™¤å»ï¼ˆå…ƒæ‰‹æ³•ãƒ™ãƒ¼ã‚¹ï¼‰"""
        try:
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image.copy()
            
            # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ–ãƒ©ãƒ¼
            blur = cv2.GaussianBlur(gray, (3, 3), 0)
            
            # äºŒå€¤åŒ–ï¼ˆå›ºå®šé–¾å€¤127ï¼‰
            _, thresh = cv2.threshold(blur, 127, 255, cv2.THRESH_BINARY_INV)
            
            # ãƒ¢ãƒ«ãƒ•ã‚©ãƒ­ã‚¸ãƒ¼ï¼ˆè»½ã‚ï¼‰
            kernel = np.ones((2, 2), np.uint8)
            opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)
            
            # åè»¢ã—ã¦èƒŒæ™¯ç™½ãƒ»æ–‡å­—é»’
            result = cv2.bitwise_not(opening)
            
            # ãƒ‡ãƒãƒƒã‚°ä¿å­˜
            if save_debug and debug_name:
                debug_file = self.debug_dir / f"guideline_removed_{debug_name}.jpg"
                cv2.imwrite(str(debug_file), result)
                logger.info(f"è£œåŠ©ç·šé™¤å»çµæœä¿å­˜: {debug_file}")
            
            return result
            
        except Exception as e:
            logger.warning(f"è£œåŠ©ç·šé™¤å»ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒç”»åƒã‚’ãã®ã¾ã¾è¿”ã™
            return image
    
    def pytorch_digit_recognition(self, image: np.ndarray, region_name: str) -> Tuple[str, float]:
        """PyTorchãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ•°å­—èªè­˜"""
        if not self.use_pytorch:
            return "", 0.0
        
        try:
            # OpenCVç”»åƒã‚’PILã«å¤‰æ›
            if len(image.shape) == 3:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(image).convert('L')
            else:
                pil_image = Image.fromarray(image).convert('L')
            
            # èƒŒæ™¯ç™½ãƒ»æ–‡å­—é»’ãªã‚‰åè»¢ï¼ˆMNISTã«åˆã‚ã›ã‚‹ï¼‰
            pil_image = ImageOps.invert(pil_image)
            
            # ä½™ç™½ã‚’è‡ªå‹•ã‚¯ãƒ­ãƒƒãƒ—
            bbox = pil_image.getbbox()
            if bbox:
                pil_image = pil_image.crop(bbox)
            
            # 28x28ã«ãƒªã‚µã‚¤ã‚º
            transform = transforms.Compose([
                transforms.Resize((28, 28)),
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (0.5,))
            ])
            
            input_tensor = transform(pil_image).unsqueeze(0)
            
            # æ¨è«–
            with torch.no_grad():
                output = self.pytorch_model(input_tensor)
                _, predicted = torch.max(output.data, 1)
                result = predicted.item()
                
                # ç¢ºä¿¡åº¦ã‚’è¨ˆç®—ï¼ˆsoftmaxï¼‰
                probabilities = torch.nn.functional.softmax(output, dim=1)
                confidence = probabilities[0][result].item()
                
                logger.info(f"{region_name}: PyTorchèªè­˜ '{result}' (ä¿¡é ¼åº¦: {confidence:.2f})")
                return str(result), confidence
                
        except Exception as e:
            logger.error(f"PyTorchèªè­˜ã‚¨ãƒ©ãƒ¼ ({region_name}): {e}")
            return "", 0.0
    
    def preprocess_number_image(self, image: np.ndarray, debug_name: str) -> List[np.ndarray]:
        """æ•°å­—ç”»åƒã®å‰å‡¦ç†ï¼ˆè¤‡æ•°ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼‰"""
        # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        
        # ç”»åƒã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ã¨ãƒªã‚µã‚¤ã‚º
        h, w = gray.shape
        if h < 30 or w < 30:
            # å°ã•ã™ãã‚‹å ´åˆã¯æ‹¡å¤§
            scale_factor = max(30 / h, 30 / w, 3.0)  # æœ€ä½3å€æ‹¡å¤§
            new_w, new_h = int(w * scale_factor), int(h * scale_factor)
            gray = cv2.resize(gray, (new_w, new_h), interpolation=cv2.INTER_CUBIC)
            logger.info(f"{debug_name}: ç”»åƒã‚’{scale_factor:.1f}å€æ‹¡å¤§ {w}x{h} -> {new_w}x{new_h}")
        
        preprocessed_images = []
        
        # å…ƒç”»åƒã‚‚è¿½åŠ ï¼ˆå‰å‡¦ç†ãªã—ï¼‰
        preprocessed_images.append(("original", gray))
        
        # 1. åŸºæœ¬çš„ãªã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        enhanced = clahe.apply(gray)
        
        # 2. ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ–ãƒ©ãƒ¼ã§ãƒã‚¤ã‚ºé™¤å»
        blurred = cv2.GaussianBlur(enhanced, (3, 3), 0)
        
        # 3. è¤‡æ•°ã®äºŒå€¤åŒ–æ‰‹æ³•
        binary_methods = [
            ("otsu", cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]),
            #("adaptive_mean", cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)),
            #("adaptive_gaussian", cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)),
            ("manual_light", cv2.threshold(blurred, 150, 255, cv2.THRESH_BINARY)[1]),
            ("manual_dark", cv2.threshold(blurred, 80, 255, cv2.THRESH_BINARY)[1])
        ]
        
        for method_name, binary in binary_methods:
            # ç©ºã®ç”»åƒãƒã‚§ãƒƒã‚¯
            if binary is None or binary.size == 0:
                continue
            
            # 4. è»½ã„ãƒ¢ãƒ«ãƒ•ã‚©ãƒ­ã‚¸ãƒ¼å‡¦ç†
            kernel = np.ones((2,2), np.uint8)
            cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            
            # ãƒ”ã‚¯ã‚»ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
            white_pixels = cv2.countNonZero(cleaned)
            if white_pixels < 10:  # ç™½ãƒ”ã‚¯ã‚»ãƒ«ãŒå°‘ãªã™ãã‚‹
                continue
            
            preprocessed_images.append((f"{method_name}", cleaned))
            
            # ãƒ‡ãƒãƒƒã‚°ä¿å­˜
            debug_file = self.debug_dir / f"improved_debug_{debug_name}_{method_name}.jpg"
            cv2.imwrite(str(debug_file), cleaned)
        
        return preprocessed_images
    
    def perform_enhanced_number_ocr(self, image: np.ndarray, region_name: str) -> Tuple[str, float]:
        """å¼·åŒ–ã•ã‚ŒãŸæ•°å­—OCRï¼ˆPyTorchãƒ¢ãƒ‡ãƒ«å„ªå…ˆï¼‰"""
        
        # PyTorchãƒ¢ãƒ‡ãƒ«ã‚’æœ€åˆã«è©¦ã™
        if self.use_pytorch:
            pytorch_result, pytorch_conf = self.pytorch_digit_recognition(image, region_name)
            if pytorch_result and pytorch_conf > 0.3:  # ä¿¡é ¼åº¦é–¾å€¤
                logger.info(f"{region_name}: PyTorchæˆåŠŸ '{pytorch_result}' (ä¿¡é ¼åº¦: {pytorch_conf:.2f})")
                return pytorch_result, pytorch_conf
            else:
                logger.info(f"{region_name}: PyTorchå¤±æ•—ã€Tesseractã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        
        # Tesseractãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        # è¤‡æ•°ã®å‰å‡¦ç†ç”»åƒã‚’ç”Ÿæˆ
        preprocessed_images = self.preprocess_number_image(image, region_name.replace(' ', '_'))
        
        # è¤‡æ•°ã®Tesseractè¨­å®šï¼ˆç·©ã„è¨­å®šã‚’è¿½åŠ ï¼‰
        ocr_configs = [
            r'--oem 3 --psm 10 -c tessedit_char_whitelist=0123456789',  # å˜ä¸€æ–‡å­—ãƒ¢ãƒ¼ãƒ‰
            r'--oem 3 --psm 8 -c tessedit_char_whitelist=0123456789',   # å˜èªãƒ¢ãƒ¼ãƒ‰
            r'--oem 3 --psm 7 -c tessedit_char_whitelist=0123456789',   # å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆè¡Œ
            r'--oem 3 --psm 6',  # ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆãªã—
            r'--oem 1 --psm 10 -c tessedit_char_whitelist=0123456789',  # LSTM + å˜ä¸€æ–‡å­—
            r'--oem 1 --psm 8',  # LSTM + ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆãªã—
        ]
        
        best_result = ("", 0.0)
        all_results = []
        
        # å…¨ã¦ã®çµ„ã¿åˆã‚ã›ã‚’è©¦ã™
        for method_name, processed_image in preprocessed_images:
            for config in ocr_configs:
                try:
                    # ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
                    text = pytesseract.image_to_string(processed_image, config=config).strip()
                    
                    # æ•°å­—ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
                    import re
                    
                    # ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚‚è¨˜éŒ²ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    if text and len(text.strip()) > 0:
                        logger.debug(f"{region_name} [{method_name}]: ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ='{text.strip()}'")
                    
                    digits_only = re.sub(r'[^0-9]', '', text)
                    
                    if digits_only:
                        # ä¿¡é ¼åº¦å–å¾—
                        try:
                            data = pytesseract.image_to_data(processed_image, config=config, output_type=pytesseract.Output.DICT)
                            confidences = [conf for conf in data['conf'] if conf > 0]
                            avg_conf = sum(confidences) / len(confidences) if confidences else 0
                        except:
                            avg_conf = 50  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¿¡é ¼åº¦
                        
                        result = (digits_only, avg_conf / 100.0)
                        all_results.append((method_name, config, result))
                        
                        if avg_conf > best_result[1] * 100:
                            best_result = result
                            logger.info(f"{region_name}: '{digits_only}' (ä¿¡é ¼åº¦: {avg_conf:.1f}) [{method_name}]")
                    
                    # æ•°å­—ãŒãªãã¦ã‚‚ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯è¨˜éŒ²
                    elif text and len(text.strip()) > 0:
                        logger.debug(f"{region_name} [{method_name}]: æ•°å­—ä»¥å¤–='{text.strip()}'")
                
                except Exception as e:
                    logger.debug(f"{region_name} [{method_name}] OCRã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        # è¨˜å…¥è€…ç•ªå·ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆ"No. 3" -> "3"ï¼‰
        if region_name == "è¨˜å…¥è€…ç•ªå·" and not best_result[0]:
            # "No."ã‚’é™¤ã„ãŸå‡¦ç†ã‚’è©¦ã™
            for method_name, processed_image in preprocessed_images:
                try:
                    # ã‚ˆã‚Šå¯›å®¹ãªè¨­å®š
                    config = r'--oem 3 --psm 6'
                    text = pytesseract.image_to_string(processed_image, config=config).strip()
                    
                    # "No. X" ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
                    import re
                    match = re.search(r'(?:No\.?\s*)?(\d+)', text, re.IGNORECASE)
                    if match:
                        digit = match.group(1)
                        best_result = (digit, 0.8)  # é«˜ä¿¡é ¼åº¦ã‚’è¨­å®š
                        logger.info(f"{region_name}: '{digit}' (ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ) [{method_name}]")
                        break
                
                except Exception:
                    continue
        
        logger.info(f"{region_name} æœ€çµ‚çµæœ: '{best_result[0]}' (ä¿¡é ¼åº¦: {best_result[1]:.2f}) [Tesseract]")
        return best_result
    
    def detect_score_and_comment_boxes(self, gray: np.ndarray, debug=False) -> Tuple[List[Tuple[int, int, int, int]], List[Tuple[int, int, int, int]]]:
        """page_split.pyã®ç‚¹æ•°ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯"""
        H, W = gray.shape
        roi_x0 = int(W * 0.6)  # å³ 40% ã ã‘è¦‹ã‚‹
        roi = gray[:, roi_x0:]
        
        # äºŒå€¤åŒ– & å‰å‡¦ç†
        th = cv2.adaptiveThreshold(roi, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 6)
        th = cv2.morphologyEx(th, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8), 1)
        th = cv2.dilate(th, np.ones((3, 3), np.uint8), 1)
        
        # è¼ªéƒ­
        cnts, _ = cv2.findContours(th, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        
        score_cand, cmt_cand = [], []
        for c in cnts:
            x, y, w, h = cv2.boundingRect(c)
            if w < 20 or h < 30:  # å°ã•ã™ãã‚‹ã‚‚ã®é™¤å¤–
                continue
            
            ratio = w / h
            area = w * h
            if area < 600:  # 30Ã—30
                continue
            
            # (x,y) ã‚’ãƒ•ãƒ«ç”»åƒåº§æ¨™ç³»ã«ç›´ã™
            X = x + roi_x0
            
            # ç‚¹æ•°å€™è£œ: æ­£æ–¹å½¢ã€œã‚„ã‚„ç¸¦é•·
            if 0.8 < ratio < 1.3:
                score_cand.append((X, y, w, h))
            # ã‚³ãƒ¡ãƒ³ãƒˆå€™è£œ: æ¨ªé•·
            elif ratio > 4.5:
                cmt_cand.append((X, y, w, h))
        
        if not score_cand or not cmt_cand:
            raise RuntimeError("ç‚¹æ•°æ  or ã‚³ãƒ¡ãƒ³ãƒˆæ ãŒæ¤œå‡ºã§ãã¾ã›ã‚“")
        
        if debug:
            dbg_roi = cv2.cvtColor(th, cv2.COLOR_GRAY2BGR)
            for X, Y, w, h in score_cand:
                cv2.rectangle(dbg_roi, (X - roi_x0, Y), (X - roi_x0 + w, Y + h), (0, 255, 255), 1)
            cv2.imwrite(str(self.debug_dir / "dbg_score_candidates.jpg"), dbg_roi)
        
        # å³ç«¯åˆ—ã ã‘æ®‹ã™
        def keep_rightmost(boxes, tol=25):
            max_x = max(b[0] for b in boxes)
            return [b for b in boxes if abs(b[0] - max_x) < tol]
        
        score_cand = keep_rightmost(score_cand, 40)  # 40px ä»¥å†…
        cmt_cand = keep_rightmost(cmt_cand)
        
        # y æ˜‡é †ã« 12 å€‹ãšã¤ãã‚ãˆã‚‹
        score_cand = sorted(score_cand, key=lambda b: b[1])[:12]
        cmt_cand = sorted(cmt_cand, key=lambda b: b[1])[:12]
        
        # margin ã‚’å†…å´ã¸å…¥ã‚Œã¦ (x1,y1,x2,y2) ã«å¤‰æ›
        def to_box(lst, margin=4):
            return [(x+margin, y+margin, x+w-margin, y+h-margin) for (x,y,w,h) in lst]
        
        score_boxes = to_box(score_cand)
        cmt_boxes = to_box(cmt_cand)
        
        # ãƒ‡ãƒãƒƒã‚°æç”»
        if debug:
            dbg_img = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
            for x1,y1,x2,y2 in score_boxes:
                cv2.rectangle(dbg_img, (x1,y1), (x2,y2), (0,0,255), 2)  # èµ¤=ç‚¹æ•°
            for x1,y1,x2,y2 in cmt_boxes:
                cv2.rectangle(dbg_img, (x1,y1), (x2,y2), (255,0,0), 2)  # é’=ã‚³ãƒ¡ãƒ³ãƒˆ
            cv2.imwrite(str(self.debug_dir / "dbg_score_comment_boxes.jpg"), dbg_img)
        
        if len(score_boxes) != 12 or len(cmt_boxes) != 12:
            raise RuntimeError(f"æ¤œå‡ºæ•°  score:{len(score_boxes)}  cmt:{len(cmt_boxes)}")
        
        return score_boxes, cmt_boxes
    
    def extract_writer_id_region(self, corrected_image: np.ndarray) -> np.ndarray:
        """page_split.pyãƒ™ãƒ¼ã‚¹ã®è¨˜å…¥è€…ç•ªå·æŠ½å‡º"""
        height, width = corrected_image.shape[:2]
        # å›ºå®šæ¯”ç‡ã§è¨˜å…¥è€…ç•ªå·é ˜åŸŸã‚’æŠ½å‡º
        x, y, w, h = (0.73, 0.03, 0.22, 0.05)
        x1, y1 = int(width * x), int(height * y)
        x2, y2 = int(width * (x + w)), int(height * (y + h))
        return corrected_image[y1:y2, x1:x2]
    
    def extract_number_regions_from_original(self, original_image: np.ndarray, debug=False) -> Dict:
        """å‹•çš„ç‚¹æ•°ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ æŠ½å‡º + è¨˜å…¥è€…ç•ªå·æŠ½å‡º"""
        gray = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY) if len(original_image.shape) == 3 else original_image
        
        # é€è¦–å¤‰æ›é©ç”¨
        corrected = self.correct_perspective(original_image, debug)
        corrected_gray = cv2.cvtColor(corrected, cv2.COLOR_BGR2GRAY) if len(corrected.shape) == 3 else corrected
        
        number_results = {
            "writer_number": {},
            "evaluations": {},
            "comments": {}
        }
        
        try:
            # è¨˜å…¥è€…ç•ªå·æŠ½å‡º
            writer_region = self.extract_writer_id_region(corrected_gray)
            cv2.imwrite(str(self.debug_dir / "improved_writer_id.jpg"), writer_region)
            text, confidence = self.perform_enhanced_number_ocr(writer_region, "è¨˜å…¥è€…ç•ªå·")
            number_results["writer_number"] = {
                "text": text,
                "confidence": confidence,
                "bbox": "dynamic"
            }
            
            # ç‚¹æ•°ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ æ¤œå‡º
            score_boxes, cmt_boxes = self.detect_score_and_comment_boxes(corrected_gray, debug)
            
            # ç‚¹æ•°æ å‡¦ç†
            eval_names = ["ç™½è©•ä¾¡1", "é»’è©•ä¾¡1", "å ´è©•ä¾¡1", "å½¢è©•ä¾¡1",
                         "ç™½è©•ä¾¡2", "é»’è©•ä¾¡2", "å ´è©•ä¾¡2", "å½¢è©•ä¾¡2",
                         "ç™½è©•ä¾¡3", "é»’è©•ä¾¡3", "å ´è©•ä¾¡3", "å½¢è©•ä¾¡3"]
            
            for idx, (x1, y1, x2, y2) in enumerate(score_boxes):
                if idx < len(eval_names):
                    name = eval_names[idx]
                    region_image = corrected_gray[y1:y2, x1:x2]
                    score_file = self.debug_dir / f"improved_score_{idx+1}.jpg"
                    cv2.imwrite(str(score_file), region_image)
                    text, confidence = self.perform_enhanced_number_ocr(region_image, name)
                    number_results["evaluations"][name] = {
                        "text": text,
                        "confidence": confidence,
                        "bbox": (x1, y1, x2, y2)
                    }
            
            # ã‚³ãƒ¡ãƒ³ãƒˆæ å‡¦ç†
            comment_names = ["ç™½ã‚³ãƒ¡ãƒ³ãƒˆ1", "é»’ã‚³ãƒ¡ãƒ³ãƒˆ1", "å ´ã‚³ãƒ¡ãƒ³ãƒˆ1", "å½¢ã‚³ãƒ¡ãƒ³ãƒˆ1",
                           "ç™½ã‚³ãƒ¡ãƒ³ãƒˆ2", "é»’ã‚³ãƒ¡ãƒ³ãƒˆ2", "å ´ã‚³ãƒ¡ãƒ³ãƒˆ2", "å½¢ã‚³ãƒ¡ãƒ³ãƒˆ2",
                           "ç™½ã‚³ãƒ¡ãƒ³ãƒˆ3", "é»’ã‚³ãƒ¡ãƒ³ãƒˆ3", "å ´ã‚³ãƒ¡ãƒ³ãƒˆ3", "å½¢ã‚³ãƒ¡ãƒ³ãƒˆ3"]
            
            for idx, (x1, y1, x2, y2) in enumerate(cmt_boxes):
                if idx < len(comment_names):
                    name = comment_names[idx]
                    region_image = corrected_gray[y1:y2, x1:x2]
                    comment_file = self.debug_dir / f"improved_comment_{idx+1}.jpg"
                    cv2.imwrite(str(comment_file), region_image)
                    # ã‚³ãƒ¡ãƒ³ãƒˆã¯OCRã—ãªã„ã§ä¿å­˜ã®ã¿
                    number_results["comments"][name] = {
                        "bbox": (x1, y1, x2, y2),
                        "image_saved": str(comment_file)
                    }
        
        except Exception as e:
            logger.warning(f"å‹•çš„æ¤œå‡ºå¤±æ•—: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç©ºã®çµæœã‚’è¿”ã™
        
        return number_results
    
    def process_form(self, image_path: str, debug=False) -> Dict:
        """æ”¹è‰¯ç‰ˆè¨˜å…¥ç”¨ç´™å‡¦ç†ï¼ˆpage_split.pyãƒ­ã‚¸ãƒƒã‚¯çµ±åˆç‰ˆï¼‰"""
        logger.info(f"æ”¹è‰¯ç‰ˆå‡¦ç†é–‹å§‹: {image_path}")
        
        # ç”»åƒèª­ã¿è¾¼ã¿
        original_image = cv2.imread(image_path)
        if original_image is None:
            raise FileNotFoundError(f"ç”»åƒãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“: {image_path}")
        
        logger.info(f"å…ƒç”»åƒã‚µã‚¤ã‚º: {original_image.shape}")
        
        # é€è¦–å¤‰æ›é©ç”¨
        corrected_image = self.correct_perspective(original_image, debug)
        cv2.imwrite(str(self.debug_dir / "improved_corrected.jpg"), corrected_image)
        
        # æ–‡å­—é ˜åŸŸæŠ½å‡ºï¼ˆå‹•çš„æ¤œå‡ºï¼‰
        character_images = self.extract_character_regions(corrected_image, debug)
        
        # æ•°å­—ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆé ˜åŸŸæŠ½å‡ºã¨OCRï¼ˆå‹•çš„æ¤œå‡ºï¼‰
        number_results = self.extract_number_regions_from_original(original_image, debug)
        
        # çµæœçµ±åˆ
        results = {
            "correction_applied": True,
            "character_recognition": character_images,  # Geminièªè­˜çµæœå«ã‚€
            "writer_number": number_results["writer_number"],
            "evaluations": number_results["evaluations"],
            "comments": number_results["comments"],
            "gemini_enabled": self.use_gemini,
            "pytorch_enabled": self.use_pytorch
        }
        
        logger.info("æ”¹è‰¯ç‰ˆå‡¦ç†å®Œäº†")
        return results


def main():
    """æ”¹è‰¯ç‰ˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆpage_split.pyçµ±åˆ + Geminièªè­˜ï¼‰"""
    
    logging.basicConfig(level=logging.INFO)
    
    # Geminiåˆ©ç”¨å¯å¦ã‚’ãƒã‚§ãƒƒã‚¯
    use_gemini = os.getenv('GEMINI_API_KEY') is not None
    if use_gemini:
        print("ğŸš€ Gemini APIæœ‰åŠ¹åŒ–: æ–‡å­—èªè­˜ã«Geminiã‚’ä½¿ç”¨")
    else:
        print("âš ï¸ Gemini APIç„¡åŠ¹: .envãƒ•ã‚¡ã‚¤ãƒ«ã«GEMINI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„")
    
    # PyTorchãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯å¦ã‚’ãƒã‚§ãƒƒã‚¯
    pytorch_model_path = "/workspace/data/digit_model.pt"
    if os.path.exists(pytorch_model_path):
        print("ğŸ§  PyTorchæ•°å­—èªè­˜ãƒ¢ãƒ‡ãƒ«æœ‰åŠ¹åŒ–: é«˜ç²¾åº¦æ•°å­—èªè­˜ã‚’ä½¿ç”¨")
    else:
        print("âš ï¸ PyTorchãƒ¢ãƒ‡ãƒ«ç„¡åŠ¹: digit_model.ptãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    processor = ImprovedOCRProcessor(use_gemini=use_gemini)
    
    try:
        results = processor.process_form("docs/è¨˜å…¥sample.JPG", debug=True)
        
        print("\n=== æ”¹è‰¯ç‰ˆå‡¦ç†çµæœï¼ˆpage_split.pyçµ±åˆ + Geminièªè­˜ï¼‰ ===")
        print(f"æ­ªã¿è£œæ­£: {'é©ç”¨' if results['correction_applied'] else 'æœªé©ç”¨'}")
        
        print("\n[æ–‡å­—ç”»åƒæŠ½å‡ºãƒ»èªè­˜ï¼ˆå‹•çš„æ¤œå‡º + Geminiï¼‰]")
        for char_name, char_data in results["character_recognition"].items():
            print(f"  {char_name}: debug/improved_char_{char_name}.jpg ä¿å­˜æ¸ˆã¿")
            
            # Geminièªè­˜çµæœã‚’è¡¨ç¤º
            gemini_result = char_data.get("gemini_recognition", {})
            if gemini_result.get("character"):
                char = gemini_result["character"]
                conf = gemini_result["confidence"]
                status = "âœ“" if conf > 0.5 else "âš "
                print(f"    â†’ Geminièªè­˜: '{char}' (ä¿¡é ¼åº¦: {conf:.2f}) {status}")
                
                # ä»£æ›¿å€™è£œãŒã‚ã‚Œã°è¡¨ç¤º
                alternatives = gemini_result.get("alternatives", [])
                if alternatives:
                    print(f"    â†’ ä»£æ›¿å€™è£œ: {', '.join(alternatives[:3])}")
            else:
                method = gemini_result.get("method", "unknown")
                print(f"    â†’ Geminièªè­˜: å¤±æ•— ({method})")
        
        print("\n[è¨˜å…¥è€…ç•ªå·ï¼ˆå‹•çš„æ¤œå‡ºï¼‰]")
        writer_data = results["writer_number"]
        if writer_data:
            status = "âœ“" if writer_data["text"] else "âœ—"
            print(f"  è¨˜å…¥è€…ç•ªå·: '{writer_data['text']}' (ä¿¡é ¼åº¦: {writer_data['confidence']:.2f}) {status}")
        
        print("\n[è©•ä¾¡æ•°å­—ï¼ˆå‹•çš„æ¤œå‡ºï¼‰]")
        for eval_name, eval_data in results["evaluations"].items():
            status = "âœ“" if eval_data["text"] else "âœ—"
            print(f"  {eval_name}: '{eval_data['text']}' (ä¿¡é ¼åº¦: {eval_data['confidence']:.2f}) {status}")
        
        print("\n[ã‚³ãƒ¡ãƒ³ãƒˆæ ï¼ˆå‹•çš„æ¤œå‡ºï¼‰]")
        for comment_name, comment_data in results["comments"].items():
            saved_path = Path(comment_data['image_saved'])
            print(f"  {comment_name}: {saved_path} ä¿å­˜æ¸ˆã¿")
        
        print(f"\næ”¹è‰¯ç‰ˆå‡¦ç†å®Œäº†ï¼ï¼ˆå‹•çš„æ¤œå‡º + Geminiæ–‡å­—èªè­˜ + PyTorchæ•°å­—èªè­˜çµ±åˆï¼‰")
        print(f"Gemini APIä½¿ç”¨: {'âœ…' if results.get('gemini_enabled') else 'âŒ'}")
        print(f"PyTorchæ•°å­—èªè­˜ä½¿ç”¨: {'âœ…' if results.get('pytorch_enabled') else 'âŒ'}")
        
    except Exception as e:
        logger.error(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()