"""
æ”¹è‰¯ç‰ˆOCR: page_split.pyã®é«˜ç²¾åº¦æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯çµ±åˆç‰ˆ
"""
import cv2
import numpy as np
import pytesseract
from typing import Dict, List, Tuple, Optional
import logging
import os
from pathlib import Path
from dotenv import load_dotenv
import torch
from PIL import Image, ImageOps
import torchvision.transforms as transforms

# .env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

logger = logging.getLogger(__name__)

# Gemini ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from .gemini_client import GeminiCharacterRecognizer
    GEMINI_AVAILABLE = True
except ImportError:
    try:
        from gemini_client import GeminiCharacterRecognizer
        GEMINI_AVAILABLE = True
    except ImportError:
        GEMINI_AVAILABLE = False
        logger.warning("Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

logger = logging.getLogger(__name__)

class SimpleCNN(torch.nn.Module):
    """MNIST+æ‰‹æ›¸ãæ•°å­—èªè­˜ç”¨ã®CNNãƒ¢ãƒ‡ãƒ«"""
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = torch.nn.Conv2d(1, 16, 3, 1)
        self.fc1 = torch.nn.Linear(26*26*16, 128)
        self.fc2 = torch.nn.Linear(128, 11)  # 0ã€œ10

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = x.view(-1, 26*26*16)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class ImprovedOCRProcessor:
    """æ”¹è‰¯ç‰ˆOCRå‡¦ç†ã‚¯ãƒ©ã‚¹ï¼ˆpage_split.pyãƒ­ã‚¸ãƒƒã‚¯çµ±åˆç‰ˆï¼‰"""
    
    def __init__(self, use_gemini=True, debug_dir="debug"):
        self.tombo_size_range = (8, 30)
        self.tombo_area_range = (50, 900)
        self.corrected_image = None
        self.corrected_width = None
        self.corrected_height = None
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        self.debug_dir = Path(debug_dir)
        self.debug_dir.mkdir(parents=True, exist_ok=True)
        
        # Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.gemini_client = None
        self.use_gemini = use_gemini and GEMINI_AVAILABLE
        
        if self.use_gemini:
            try:
                self.gemini_client = GeminiCharacterRecognizer()
                logger.info("Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"Gemini API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•—: {e}")
                self.use_gemini = False
        
        # PyTorchãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.pytorch_model = None
        self.use_pytorch = False
        try:
            self.pytorch_model = SimpleCNN()
            model_path = "/workspace/data/digit_model.pt"
            if not os.path.exists(model_path):
                logger.error(f"PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
                logger.error(f"ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
                logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹: {os.listdir('.') if os.path.exists('.') else 'N/A'}")
                if os.path.exists('data'):
                    logger.error(f"dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹: {os.listdir('data')}")
                else:
                    logger.error("dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                self.pytorch_model = None
                return
            if os.path.exists(model_path):
                self.pytorch_model.load_state_dict(torch.load(model_path, map_location='cpu'))
                self.pytorch_model.eval()
                self.use_pytorch = True
                logger.info("PyTorchæ•°å­—èªè­˜ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æˆåŠŸ")
            else:
                logger.warning(f"PyTorchãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        except Exception as e:
            logger.warning(f"PyTorchãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
            self.use_pytorch = False
    
    def find_page_corners(self, gray: np.ndarray) -> Optional[np.ndarray]:
        """æ”¹è‰¯ç‰ˆãƒšãƒ¼ã‚¸æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯"""
        try:
            # è¤‡æ•°ã®æ‰‹æ³•ã§4éš…æ¤œå‡ºã‚’è©¦è¡Œ
            h, w = gray.shape
            min_area = (h * w) * 0.3  # æœ€å°é¢ç©ã‚’30%ã«è¨­å®š
            
            # æ‰‹æ³•1: ã‚¨ãƒƒã‚¸æ¤œå‡º + è¼ªéƒ­æŠ½å‡º
            edges = cv2.Canny(gray, 50, 150, apertureSize=3)
            kernel = np.ones((3, 3), np.uint8)
            edges = cv2.dilate(edges, kernel, iterations=1)
            
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # é¢ç©ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½å€™è£œã‚’æ¤œè¨
            contours = sorted(contours, key=cv2.contourArea, reverse=True)
            
            for contour in contours[:5]:  # ä¸Šä½5ã¤ã‚’è©¦è¡Œ
                area = cv2.contourArea(contour)
                if area < min_area:
                    continue
                    
                peri = cv2.arcLength(contour, True)
                # ã‚ˆã‚Šå³å¯†ãªè¿‘ä¼¼ï¼ˆ0.01ã«å¤‰æ›´ï¼‰
                approx = cv2.approxPolyDP(contour, 0.01 * peri, True)
                
                if len(approx) == 4:
                    # 4éš…ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                    corners = approx.reshape(4, 2).astype(np.float32)
                    if self._validate_corners(corners, w, h):
                        return self._order_corners(corners)
            
            # æ‰‹æ³•2: ã‚ˆã‚Šç·©ã„è¿‘ä¼¼ã§ã®å†è©¦è¡Œ
            for contour in contours[:3]:
                area = cv2.contourArea(contour)
                if area < min_area:
                    continue
                    
                peri = cv2.arcLength(contour, True)
                approx = cv2.approxPolyDP(contour, 0.03 * peri, True)  # ã‚ˆã‚Šç·©ã„è¿‘ä¼¼
                
                if len(approx) >= 4:
                    # æœ€ã‚‚å¤–å´ã®4ç‚¹ã‚’é¸æŠ
                    corners = self._select_outer_corners(approx.reshape(-1, 2), w, h)
                    if self._validate_corners(corners, w, h):
                        return self._order_corners(corners)
            
            return None
            
        except Exception as e:
            logger.warning(f"ãƒšãƒ¼ã‚¸æ¤œå‡ºå¤±æ•—: {e}")
            return None
    
    def _validate_corners(self, corners: np.ndarray, w: int, h: int) -> bool:
        """4éš…ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            # æœ€å°ã®å››è§’å½¢é¢ç©ãƒã‚§ãƒƒã‚¯
            area = cv2.contourArea(corners)
            min_area = (w * h) * 0.3
            if area < min_area:
                return False
            
            # è§’åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆæ¥µç«¯ã«é‹­è§’ãƒ»éˆè§’ã§ãªã„ã‹ï¼‰
            for i in range(4):
                p1 = corners[i]
                p2 = corners[(i + 1) % 4] 
                p3 = corners[(i + 2) % 4]
                
                v1 = p2 - p1
                v2 = p3 - p2
                
                # ãƒ™ã‚¯ãƒˆãƒ«ã®å†…ç©ã‹ã‚‰è§’åº¦ã‚’è¨ˆç®—
                cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
                angle = np.arccos(np.clip(cos_angle, -1, 1)) * 180 / np.pi
                
                # 30åº¦ã€œ150åº¦ã®ç¯„å›²ã§ãªã„å ´åˆã¯ç„¡åŠ¹
                if angle < 30 or angle > 150:
                    return False
            
            return True
        except:
            return False
    
    def _select_outer_corners(self, points: np.ndarray, w: int, h: int) -> np.ndarray:
        """è¤‡æ•°ç‚¹ã‹ã‚‰æœ€ã‚‚å¤–å´ã®4ç‚¹ã‚’é¸æŠ"""
        try:
            # å·¦ä¸Šãƒ»å³ä¸Šãƒ»å³ä¸‹ãƒ»å·¦ä¸‹ã‚’æ¢ã™
            top_left = min(points, key=lambda p: p[0] + p[1])
            top_right = min(points, key=lambda p: (w - p[0]) + p[1])
            bottom_right = min(points, key=lambda p: (w - p[0]) + (h - p[1]))
            bottom_left = min(points, key=lambda p: p[0] + (h - p[1]))
            
            return np.array([top_left, top_right, bottom_right, bottom_left], dtype=np.float32)
        except:
            return points[:4].astype(np.float32)
    
    def _order_corners(self, corners: np.ndarray) -> np.ndarray:
        """4éš…ã‚’æ™‚è¨ˆå›ã‚Šé †ã«ä¸¦ã³æ›¿ãˆï¼ˆå·¦ä¸Šâ†’å³ä¸Šâ†’å³ä¸‹â†’å·¦ä¸‹ï¼‰"""
        try:
            # é‡å¿ƒã‚’è¨ˆç®—
            center = corners.mean(axis=0)
            
            # å„ç‚¹ã¨é‡å¿ƒã®ç›¸å¯¾ä½ç½®ã§åˆ†é¡
            ordered = np.zeros((4, 2), dtype=np.float32)
            
            for corner in corners:
                if corner[0] < center[0] and corner[1] < center[1]:  # å·¦ä¸Š
                    ordered[0] = corner
                elif corner[0] > center[0] and corner[1] < center[1]:  # å³ä¸Š
                    ordered[1] = corner
                elif corner[0] > center[0] and corner[1] > center[1]:  # å³ä¸‹
                    ordered[2] = corner
                else:  # å·¦ä¸‹
                    ordered[3] = corner
            
            return ordered
        except:
            return corners
    
    def order_corners(self, pts: np.ndarray) -> np.ndarray:
        """4ç‚¹ã‚’TL, TR, BR, BLé †ã«ä¸¦ã¹ã‚‹"""
        pts = pts[np.argsort(pts[:, 0])]  # x ã§å·¦2 / å³2
        left, right = pts[:2], pts[2:]
        tl = left[np.argmin(left[:, 1])]
        bl = left[np.argmax(left[:, 1])]
        tr = right[np.argmin(right[:, 1])]
        br = right[np.argmax(right[:, 1])]
        return np.array([tl, tr, br, bl], dtype="float32")
    
    def perspective_correct_advanced(self, img: np.ndarray, corners: np.ndarray, dbg=False) -> np.ndarray:
        """page_split.pyã®é€è¦–å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯"""
        pts = self.order_corners(corners)
        
        wA = np.linalg.norm(pts[2] - pts[3])
        wB = np.linalg.norm(pts[1] - pts[0])
        hA = np.linalg.norm(pts[1] - pts[2])
        hB = np.linalg.norm(pts[0] - pts[3])
        W = int(max(wA, wB))
        H = int(max(hA, hB))
        
        dst = np.array([[0, 0], [W - 1, 0], [W - 1, H - 1], [0, H - 1]], dtype="float32")
        M = cv2.getPerspectiveTransform(pts, dst)
        warped = cv2.warpPerspective(img, M, (W, H))
        
        if dbg:
            dbg_img = img.copy()
            for (x, y) in pts.astype(int):
                cv2.circle(dbg_img, (x, y), 10, (0, 0, 255), -1)
            cv2.imwrite(str(self.debug_dir / "dbg_corners.jpg"), dbg_img)
            cv2.imwrite(str(self.debug_dir / "dbg_warped.jpg"), warped)
        
        return warped
    
    def correct_perspective(self, image: np.ndarray, debug=False) -> np.ndarray:
        """A4ç”»åƒå…¨ä½“ç…§æ˜è£œæ­£ + page_split.pyãƒ™ãƒ¼ã‚¹ã®é€è¦–å¤‰æ›"""
        
        # Step 0: A4ç”»åƒå…¨ä½“ã«ç…§æ˜ãƒ ãƒ©è£œæ­£ã‚’æœ€åˆã«é©ç”¨
        lighting_corrected_image = self.apply_lighting_correction(image)
        logger.info("A4ç”»åƒå…¨ä½“ã«ç…§æ˜ãƒ ãƒ©è£œæ­£ã‚’é©ç”¨")
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨ç…§æ˜è£œæ­£ç”»åƒä¿å­˜
        if debug:
            debug_lighting = self.debug_dir / "a4_lighting_corrected.jpg"
            cv2.imwrite(str(debug_lighting), lighting_corrected_image)
        
        gray = cv2.cvtColor(lighting_corrected_image, cv2.COLOR_BGR2GRAY) if len(lighting_corrected_image.shape) == 3 else lighting_corrected_image
        
        # ãƒšãƒ¼ã‚¸æ¤œå‡ºã‚’è©¦è¡Œï¼ˆç…§æ˜è£œæ­£æ¸ˆã¿ç”»åƒã§å®Ÿè¡Œï¼‰
        corners = self.find_page_corners(gray)
        
        if corners is not None:
            # é€è¦–å¤‰æ›å®Ÿè¡Œï¼ˆç…§æ˜è£œæ­£æ¸ˆã¿ç”»åƒã‚’ä½¿ç”¨ï¼‰
            warped = self.perspective_correct_advanced(lighting_corrected_image, corners, debug)
            logger.info(f"é€è¦–å¤‰æ›é©ç”¨: {warped.shape}")
            return warped
        else:
            # è£œæ­£ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç…§æ˜è£œæ­£æ¸ˆã¿ç”»åƒã‚’è¿”ã™ï¼‰
            logger.info("ãƒšãƒ¼ã‚¸æ¤œå‡ºå¤±æ•—ã€ç…§æ˜è£œæ­£æ¸ˆã¿ç”»åƒã‚’ä½¿ç”¨")
            return lighting_corrected_image.copy()
    
    def detect_char_cells(self, gray: np.ndarray, debug=False) -> List[Tuple[int, int, int, int]]:
        """page_split.pyã®æ–‡å­—ã‚»ãƒ«æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯"""
        try:
            H, W = gray.shape
            
            # äºŒå€¤åŒ– â†’ ç´°ç·šã‚’å¤ªã‚‰ã›ã¦è¼ªéƒ­ã‚’é–‰ã˜ã‚‹
            th = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 15, 8)
            kernel = np.ones((3, 3), np.uint8)
            th = cv2.dilate(th, kernel, iterations=1)
            
            # è¼ªéƒ­æŠ½å‡º
            cnts, _ = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # ã‚ˆã‚Šå³å¯†ãªæ–‡å­—ã‚»ãƒ«æ¤œå‡ºãƒ•ã‚£ãƒ«ã‚¿
            cand = []
            for c in cnts:
                area = cv2.contourArea(c)
                # ã‚ˆã‚Šå³ã—ã„é¢ç©åˆ¶é™ï¼ˆæ–‡å­—ã‚»ãƒ«ã‚µã‚¤ã‚ºã«åˆã‚ã›ã‚‹ï¼‰
                min_area = (H * W) * 0.005  # 0.002 â†’ 0.005 (ã‚ˆã‚Šå¤§ããªä¸‹é™)
                max_area = (H * W) * 0.015  # 0.03 â†’ 0.015 (ã‚ˆã‚Šå°ã•ãªä¸Šé™)
                
                if area < min_area or area > max_area:
                    continue
                
                # è¼ªéƒ­ã®è¤‡é›‘ã•ãƒã‚§ãƒƒã‚¯
                peri = cv2.arcLength(c, True)
                approx = cv2.approxPolyDP(c, 0.015 * peri, True)  # ã‚ˆã‚Šå³å¯†ãªè¿‘ä¼¼
                
                # 4è§’å½¢ã¾ãŸã¯ãã‚Œã«è¿‘ã„å½¢çŠ¶
                if len(approx) < 4 or len(approx) > 6:
                    continue
                
                x, y, w, h = cv2.boundingRect(approx)
                
                # ã‚ˆã‚Šå³ã—ã„æ­£æ–¹å½¢åˆ¤å®š
                aspect_ratio = w / h
                if not (0.9 < aspect_ratio < 1.1):  # 0.85-1.15 â†’ 0.9-1.1
                    continue
                
                # ã‚µã‚¤ã‚ºã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæ–‡å­—ã‚»ãƒ«ã¨ã—ã¦é©åˆ‡ãªã‚µã‚¤ã‚ºã‹ï¼‰
                min_size = min(H, W) * 0.08  # æœ€å°ã‚µã‚¤ã‚º
                max_size = min(H, W) * 0.25  # æœ€å¤§ã‚µã‚¤ã‚º
                
                if not (min_size < w < max_size and min_size < h < max_size):
                    continue
                
                cand.append((x, y, w, h))
            
            if len(cand) < 3:
                raise RuntimeError("èª²é¡Œãƒã‚¹ã‚‰ã—ãå››è§’ãŒ 3 ã¤è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # åˆ—ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚° â†’ å³åˆ—ã‚’é¸æŠ
            xs = np.array([[c[0]] for c in cand], np.float32)
            _, labels, centers = cv2.kmeans(xs, 2, None,
                (cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0),
                10, cv2.KMEANS_PP_CENTERS)
            right_cluster = np.argmax(centers)  # x ãŒå¤§ãã„æ–¹
            right_boxes = [c for c,l in zip(cand, labels.flatten()) if l == right_cluster]
            
            # y ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šâ†’ä¸‹ 3 ã¤ã‚’å–å¾—
            right_boxes = sorted(right_boxes, key=lambda b: b[1])[:3]
            cells = []
            margin = 10
            for x, y, w, h in right_boxes:
                cells.append((x + margin, y + margin, x + w - margin, y + h - margin))
            
            if debug:
                # è©³ç´°ãƒ‡ãƒãƒƒã‚°ç”»åƒç”Ÿæˆ
                dbg = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
                
                # å…¨å€™è£œã‚’èµ¤ã§è¡¨ç¤º
                for (x, y, w, h) in cand:
                    cv2.rectangle(dbg, (x, y), (x+w, y+h), (0, 0, 255), 1)
                    cv2.putText(dbg, f"{w}x{h}", (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)
                
                # é¸æŠã•ã‚ŒãŸæ–‡å­—ã‚»ãƒ«ã‚’ç·‘ã§è¡¨ç¤º
                for i, (x1, y1, x2, y2) in enumerate(cells):
                    cv2.rectangle(dbg, (x1, y1), (x2, y2), (0, 255, 0), 3)
                    cv2.putText(dbg, f"Cell{i+1}", (x1+5, y1+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                
                # å³åˆ—ã®ãƒœãƒƒã‚¯ã‚¹ã‚’é’ã§è¡¨ç¤º
                for (x, y, w, h) in right_boxes:
                    cv2.rectangle(dbg, (x, y), (x+w, y+h), (255, 0, 0), 2)
                
                cv2.imwrite(str(self.debug_dir / "dbg_cells_contour.jpg"), dbg)
                
                # ãƒ­ã‚°å‡ºåŠ›
                print(f"[æ–‡å­—ã‚»ãƒ«æ¤œå‡º] å€™è£œæ•°: {len(cand)}, å³åˆ—å€™è£œ: {len(right_boxes)}, æœ€çµ‚é¸æŠ: {len(cells)}å€‹")
                for i, (x, y, w, h) in enumerate(cand):
                    area_ratio = (w * h) / (H * W)
                    print(f"  å€™è£œ{i+1}: ({x},{y}) {w}x{h}, é¢ç©æ¯”: {area_ratio:.4f}")
                for i, (x1, y1, x2, y2) in enumerate(cells):
                    print(f"  ã‚»ãƒ«{i+1}: ({x1},{y1})-({x2},{y2}) ã‚µã‚¤ã‚º: {x2-x1}x{y2-y1}")
            
            if len(cells) != 3:
                raise RuntimeError("èª²é¡Œãƒã‚¹ã‚’ 3 ã¤å–å¾—ã§ãã¾ã›ã‚“")
            
            return cells
        
        except Exception as e:
            logger.warning(f"æ–‡å­—ã‚»ãƒ«æ¤œå‡ºå¤±æ•—: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å›ºå®šæ¯”ç‡
            H, W = gray.shape
            W0 = int(W*0.07); W1 = int(W*0.27)
            cell_h = int(H*0.23); gap = int(H*0.31)
            cells = [(W0, int(H*0.11)+i*gap, W1, int(H*0.11)+i*gap+cell_h) for i in range(3)]
            return cells
    
    def recognize_character_with_gemini(self, image: np.ndarray, char_name: str) -> Dict[str, any]:
        """Gemini APIã‚’ä½¿ç”¨ã—ãŸæ–‡å­—èªè­˜"""
        if not self.use_gemini or self.gemini_client is None:
            return {
                "character": "",
                "confidence": 0.0,
                "method": "gemini_unavailable"
            }
        
        try:
            context = f"æ‰‹æ›¸ãç·´ç¿’ç”¨ç´™ã®æ–‡å­—æ {char_name}ã‹ã‚‰åˆ‡ã‚Šå‡ºã•ã‚ŒãŸæ‰‹æ›¸ãæ–‡å­—"            
            result = self.gemini_client.recognize_japanese_character(image, context)
            
            return {
                "character": result.get("character", ""),
                "confidence": result.get("confidence", 0.0),
                "alternatives": result.get("alternatives", []),
                "reasoning": result.get("reasoning", ""),
                "method": "gemini"
            }
            
        except Exception as e:
            logger.error(f"Geminiæ–‡å­—èªè­˜ã‚¨ãƒ©ãƒ¼ ({char_name}): {e}")
            return {
                "character": "",
                "confidence": 0.0,
                "method": "gemini_error",
                "error": str(e)
            }
    
    def extract_character_regions(self, corrected_image: np.ndarray, debug=False) -> Dict:
        """page_split.pyãƒ™ãƒ¼ã‚¹ã®æ–‡å­—é ˜åŸŸæŠ½å‡º + Geminiæ–‡å­—èªè­˜"""
        gray = cv2.cvtColor(corrected_image, cv2.COLOR_BGR2GRAY) if len(corrected_image.shape) == 3 else corrected_image
        
        # æ–‡å­—ã‚»ãƒ«æ¤œå‡º
        cells = self.detect_char_cells(gray, debug)
        
        character_results = {}
        char_names = ["char_1", "char_2", "char_3"]  # æ±ç”¨å
        
        for i, (x1, y1, x2, y2) in enumerate(cells):
            name = char_names[i]
            region_image = gray[y1:y2, x1:x2]
            
            # è£œåŠ©ç·šé™¤å»ã‚’é©ç”¨
            cleaned_region = self.remove_guidelines(region_image, save_debug=True, debug_name=name)
            
            # ãƒ‡ãƒãƒƒã‚°ä¿å­˜ï¼ˆè£œåŠ©ç·šé™¤å»å‰ï¼‰
            char_file = self.debug_dir / f"improved_char_{name}_original.jpg"
            cv2.imwrite(str(char_file), region_image)
            
            # ãƒ‡ãƒãƒƒã‚°ä¿å­˜ï¼ˆè£œåŠ©ç·šé™¤å»å¾Œï¼‰
            char_file_cleaned = self.debug_dir / f"improved_char_{name}.jpg"
            cv2.imwrite(str(char_file_cleaned), cleaned_region)
            logger.info(f"{name}æ–‡å­—é ˜åŸŸä¿å­˜ï¼ˆè£œåŠ©ç·šé™¤å»æ¸ˆã¿ï¼‰: {char_file_cleaned}")
            
            # Geminiæ–‡å­—èªè­˜ï¼ˆè£œåŠ©ç·šé™¤å»å¾Œã®ç”»åƒã‚’ä½¿ç”¨ï¼‰
            gemini_result = self.recognize_character_with_gemini(cleaned_region, name)
            
            character_results[name] = {
                "image": region_image,  # å…ƒç”»åƒ
                "cleaned_image": cleaned_region,  # è£œåŠ©ç·šé™¤å»å¾Œ
                "bbox": (x1, y1, x2, y2),
                "gemini_recognition": gemini_result
            }
            
            # èªè­˜çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
            if gemini_result["character"]:
                confidence = gemini_result["confidence"]
                char = gemini_result["character"]
                logger.info(f"{name} Geminièªè­˜: '{char}' (ä¿¡é ¼åº¦: {confidence:.2f})")
            else:
                logger.info(f"{name} Geminièªè­˜: èªè­˜å¤±æ•—")
        
        return character_results
    
    def apply_lighting_correction(self, image: np.ndarray) -> np.ndarray:
        """è»½ã„ç…§æ˜ãƒ ãƒ©è£œæ­£å‡¦ç†"""
        try:
            if len(image.shape) == 3:
                # ã‚«ãƒ©ãƒ¼ç”»åƒã®å ´åˆã¯ã€å„ãƒãƒ£ãƒ³ãƒãƒ«ã«åŒã˜è£œæ­£ã‚’é©ç”¨
                channels = cv2.split(image)
                corrected_channels = []
                
                for channel in channels:
                    # è»½ã„CLAHE (ã‚ˆã‚Šæ§ãˆã‚ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
                    clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(16, 16))
                    enhanced = clahe.apply(channel)
                    corrected_channels.append(enhanced)
                
                return cv2.merge(corrected_channels)
            else:
                # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç”»åƒ
                gray = image.copy()
                
                # è»½ã„CLAHEé©ç”¨
                clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(16, 16))
                enhanced = clahe.apply(gray)
                
                return enhanced
            
        except Exception as e:
            logger.warning(f"ç…§æ˜è£œæ­£ã‚¨ãƒ©ãƒ¼: {e}")
            return image
    
    def remove_guidelines(self, image: np.ndarray, save_debug=False, debug_name="") -> np.ndarray:
        """æ”¹è‰¯è£œåŠ©ç·šé™¤å»ï¼ˆæ–‡å­—ä¿è­·å¼·åŒ– + ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆæ”¹å–„ï¼‰"""
        try:
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
            #if len(image.shape) == 3:
            #    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            #else:
            #    gray = image.copy()
            
            # Step 1: è»½ã„ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–ï¼ˆæ–‡å­—ã‚’ã‚ˆã‚Šæ˜ç¢ºã«ï¼‰
            # enhanced = cv2.convertScaleAbs(gray, alpha=1.3, beta=-5)
            # ç”»åƒã®ä¸­å¤®å€¤ã«åŸºã¥ãå‹•çš„ãªã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´
            #median_intensity = np.median(gray)
            #alpha = 1.2 if median_intensity < 120 else 1.0
            #beta  = -10 if median_intensity < 120 else -3
            #enhanced = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)

            # Step 2: è»½ã„ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ–ãƒ©ãƒ¼ï¼ˆãƒã‚¤ã‚ºé™¤å»ï¼‰
            #blur = cv2.GaussianBlur(enhanced, (3, 3), 0)

            # Step 3: æ–‡å­—ä¿è­·ã®ãŸã‚ã®é«˜ã„é–¾å€¤è¨­å®š
            #_, thresh = cv2.threshold(blur, 130, 255, cv2.THRESH_BINARY_INV)
            # Step 4: æœ€å°é™ã®ãƒ¢ãƒ«ãƒ•ã‚©ãƒ­ã‚¸ãƒ¼å‡¦ç†ï¼ˆæ–‡å­—ã‚’éåº¦ã«å‰Šã‚‰ãªã„ï¼‰
            #kernel = np.ones((2, 2), np.uint8)
            #opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)
            # Step 5: ã»ã¨ã‚“ã©è†¨å¼µã—ãªã„ï¼ˆæ–‡å­—ã®ç´°éƒ¨ä¿è­·ï¼‰
            #kernel_dilate = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 1))
            #dilated = cv2.dilate(opening, kernel_dilate, iterations=1)
            # Step 6: åè»¢ã—ã¦èƒŒæ™¯ç™½ãƒ»æ–‡å­—é»’
            #result = cv2.bitwise_not(opening)

            # --- 1. ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ« & è»½ãå¹³æ»‘åŒ– ---
            if image.ndim == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image.copy() 
            # ç”»åƒã®ä¸­å¤®å€¤ã«åŸºã¥ãå‹•çš„ãªã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´
            median_intensity = np.median(gray)
            alpha = 1.2 if median_intensity < 120 else 1.0
            beta  = -10 if median_intensity < 120 else -3
            enhanced = cv2.convertScaleAbs(gray, alpha=alpha, beta=beta)
            blur = cv2.GaussianBlur(enhanced, (3, 3), 0)

            # --- 2. é»’å¸½ã§ã€Œæš—ãç´°ã„ç·šã€æŠ½å‡º -------------------------------
            H, W = gray.shape
            k_h  = cv2.getStructuringElement(cv2.MORPH_RECT, (max(15, W//4), 1))
            k_v  = cv2.getStructuringElement(cv2.MORPH_RECT, (1, max(15, H//4)))

            bh_h = cv2.morphologyEx(blur, cv2.MORPH_BLACKHAT, k_h)
            bh_v = cv2.morphologyEx(blur, cv2.MORPH_BLACKHAT, k_v)

            # --- 3. äºŒå€¤åŒ–ï¼ˆãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹æ°—å‘³ã«å¼±ã‚ï¼‰ ------------------------
            _, m_h = cv2.threshold(bh_h, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            _, m_v = cv2.threshold(bh_v, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            guide  = cv2.bitwise_or(m_h, m_v)
            # --- 4. é€£çµæˆåˆ†ã§ã€Œé•·ã„ã®ã«ç´°ã„ã€ã‚‚ã®ã ã‘æ®‹ã™ -------------------
            num, lbl, stats, _ = cv2.connectedComponentsWithStats(guide, 8)
            slim = np.zeros_like(guide)
            for i in range(1, num):
                w, h, area = stats[i, cv2.CC_STAT_WIDTH], stats[i, cv2.CC_STAT_HEIGHT], stats[i, cv2.CC_STAT_AREA]
            # å¹…ã‹é«˜ã•ãŒç”»é¢ã®70%ä»¥ä¸Šã€ã‹ã¤é¢ç©ç‡ < 0.15 â†’ ã‚¬ã‚¤ãƒ‰ç·š
            if ((w > 0.7*W and h < 0.1*H) or (h > 0.7*H and w < 0.1*W)) and (area/(w*h) < 0.15):
                slim[lbl == i] = 255

            # --- 5. ãƒã‚¤ãƒŠãƒªåŒ– â†’ slim ã‚’å¼•ã -------------------------------
            _, text = cv2.threshold(blur, 0, 255,
                                    cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
            clean = cv2.bitwise_and(text, cv2.bitwise_not(slim))

            # --- 6. ã»ã‚“ã®ã‚Šè†¨å¼µâ†’åç¸®ã§æ¬ ã‘ã‚’åŸ‹ã‚æˆ»ã™ -----------------------
            clean = cv2.morphologyEx(clean, cv2.MORPH_CLOSE,
                                    np.ones((2, 2), np.uint8), 1)

            return cv2.bitwise_not(clean)

            # ãƒ‡ãƒãƒƒã‚°ä¿å­˜ï¼ˆè¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
            if save_debug and debug_name and self.debug_dir:
                debug_file = self.debug_dir / f"guideline_removed_{debug_name}.jpg"
                enhanced_file = self.debug_dir / f"enhanced_{debug_name}.jpg"
                dbg_thin_file = self.debug_dir / f"dbg_thin_{debug_name}.jpg"
                dbg_guide_file = self.debug_dir / f"dbg_guide_{debug_name}.jpg"
                dbg_inpaint_file = self.debug_dir / f"dbg_inpaint_{debug_name}.jpg"
                
                cv2.imwrite(str(dbg_thin_file), thin)
                cv2.imwrite(str(dbg_guide_file), guide)
                cv2.imwrite(str(dbg_inpaint_file), inpainted)

                cv2.imwrite(str(debug_file), result)
                #cv2.imwrite(str(enhanced_file), enhanced)
                logger.info(f"æ”¹è‰¯è£œåŠ©ç·šé™¤å»çµæœä¿å­˜: {debug_file}")
            
            return result
            
        except Exception as e:
            logger.warning(f"è£œåŠ©ç·šé™¤å»ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒç”»åƒã‚’ãã®ã¾ã¾è¿”ã™
            return image
    
    def pytorch_digit_recognition(self, image: np.ndarray, region_name: str) -> Tuple[str, float]:
        """PyTorchãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ•°å­—èªè­˜ï¼ˆå…ƒã®ã‚·ãƒ³ãƒ—ãƒ«æ‰‹æ³•ï¼‰"""
        if not self.use_pytorch:
            return "", 0.0
        
        try:
            # OpenCVç”»åƒã‚’PILã«å¤‰æ›
            if len(image.shape) == 3:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(image).convert('L')
            else:
                pil_image = Image.fromarray(image).convert('L')
            
            # èƒŒæ™¯ç™½ãƒ»æ–‡å­—é»’ãªã‚‰åè»¢ï¼ˆMNISTã«åˆã‚ã›ã‚‹ï¼‰
            pil_image = ImageOps.invert(pil_image)
            
            # ä½™ç™½ã‚’è‡ªå‹•ã‚¯ãƒ­ãƒƒãƒ—
            bbox = pil_image.getbbox()
            if bbox:
                pil_image = pil_image.crop(bbox)
            
            # 28x28ã«ãƒªã‚µã‚¤ã‚º
            transform = transforms.Compose([
                transforms.Resize((28, 28)),
                transforms.ToTensor(),
                transforms.Normalize((0.5,), (0.5,))
            ])
            
            input_tensor = transform(pil_image).unsqueeze(0)
            
            # æ¨è«–
            with torch.no_grad():
                output = self.pytorch_model(input_tensor)
                _, predicted = torch.max(output.data, 1)
                result = predicted.item()
                
                # ç¢ºä¿¡åº¦ã‚’è¨ˆç®—ï¼ˆsoftmaxï¼‰
                probabilities = torch.nn.functional.softmax(output, dim=1)
                confidence = probabilities[0][result].item()
                
                logger.info(f"{region_name}: PyTorchèªè­˜ '{result}' (ä¿¡é ¼åº¦: {confidence:.2f})")
                return str(result), confidence
                
        except Exception as e:
            logger.error(f"PyTorchèªè­˜ã‚¨ãƒ©ãƒ¼ ({region_name}): {e}")
            return "", 0.0
    
    def preprocess_number_image(self, image: np.ndarray, debug_name: str) -> List[np.ndarray]:
        """æ•°å­—ç”»åƒã®å‰å‡¦ç†ï¼ˆè¤‡æ•°ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼‰"""
        # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        
        # ç”»åƒã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ã¨ãƒªã‚µã‚¤ã‚º
        h, w = gray.shape
        if h < 30 or w < 30:
            # å°ã•ã™ãã‚‹å ´åˆã¯æ‹¡å¤§
            scale_factor = max(30 / h, 30 / w, 3.0)  # æœ€ä½3å€æ‹¡å¤§
            new_w, new_h = int(w * scale_factor), int(h * scale_factor)
            gray = cv2.resize(gray, (new_w, new_h), interpolation=cv2.INTER_CUBIC)
            logger.info(f"{debug_name}: ç”»åƒã‚’{scale_factor:.1f}å€æ‹¡å¤§ {w}x{h} -> {new_w}x{new_h}")
        
        preprocessed_images = []
        
        # å…ƒç”»åƒã‚‚è¿½åŠ ï¼ˆå‰å‡¦ç†ãªã—ï¼‰
        preprocessed_images.append(("original", gray))
        
        # 1. åŸºæœ¬çš„ãªã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        enhanced = clahe.apply(gray)
        
        # 2. ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ–ãƒ©ãƒ¼ã§ãƒã‚¤ã‚ºé™¤å»
        blurred = cv2.GaussianBlur(enhanced, (3, 3), 0)
        
        # 3. è¤‡æ•°ã®äºŒå€¤åŒ–æ‰‹æ³•
        binary_methods = [
            ("otsu", cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]),
            #("adaptive_mean", cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)),
            #("adaptive_gaussian", cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)),
            ("manual_light", cv2.threshold(blurred, 150, 255, cv2.THRESH_BINARY)[1]),
            ("manual_dark", cv2.threshold(blurred, 80, 255, cv2.THRESH_BINARY)[1])
        ]
        
        for method_name, binary in binary_methods:
            # ç©ºã®ç”»åƒãƒã‚§ãƒƒã‚¯
            if binary is None or binary.size == 0:
                continue
            
            # 4. è»½ã„ãƒ¢ãƒ«ãƒ•ã‚©ãƒ­ã‚¸ãƒ¼å‡¦ç†
            kernel = np.ones((2,2), np.uint8)
            cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            
            # ãƒ”ã‚¯ã‚»ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
            white_pixels = cv2.countNonZero(cleaned)
            if white_pixels < 10:  # ç™½ãƒ”ã‚¯ã‚»ãƒ«ãŒå°‘ãªã™ãã‚‹
                continue
            
            preprocessed_images.append((f"{method_name}", cleaned))
            
            # ãƒ‡ãƒãƒƒã‚°ä¿å­˜
            debug_file = self.debug_dir / f"improved_debug_{debug_name}_{method_name}.jpg"
            cv2.imwrite(str(debug_file), cleaned)
        
        return preprocessed_images
    
    def perform_enhanced_number_ocr(self, image: np.ndarray, region_name: str) -> Tuple[str, float]:
        """å¼·åŒ–ã•ã‚ŒãŸæ•°å­—OCRï¼ˆPyTorchãƒ¢ãƒ‡ãƒ«å„ªå…ˆï¼‰"""
        
        # PyTorchãƒ¢ãƒ‡ãƒ«ã‚’æœ€åˆã«è©¦ã™
        if self.use_pytorch:
            pytorch_result, pytorch_conf = self.pytorch_digit_recognition(image, region_name)
            if pytorch_result and pytorch_conf > 0.3:  # ä¿¡é ¼åº¦é–¾å€¤
                logger.info(f"{region_name}: PyTorchæˆåŠŸ '{pytorch_result}' (ä¿¡é ¼åº¦: {pytorch_conf:.2f})")
                return pytorch_result, pytorch_conf
            else:
                logger.info(f"{region_name}: PyTorchå¤±æ•—ã€Tesseractã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        
        # Tesseractãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        # è¤‡æ•°ã®å‰å‡¦ç†ç”»åƒã‚’ç”Ÿæˆ
        preprocessed_images = self.preprocess_number_image(image, region_name.replace(' ', '_'))
        
        # è¤‡æ•°ã®Tesseractè¨­å®šï¼ˆç·©ã„è¨­å®šã‚’è¿½åŠ ï¼‰
        ocr_configs = [
            r'--oem 3 --psm 10 -c tessedit_char_whitelist=0123456789',  # å˜ä¸€æ–‡å­—ãƒ¢ãƒ¼ãƒ‰
            r'--oem 3 --psm 8 -c tessedit_char_whitelist=0123456789',   # å˜èªãƒ¢ãƒ¼ãƒ‰
            r'--oem 3 --psm 7 -c tessedit_char_whitelist=0123456789',   # å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆè¡Œ
            r'--oem 3 --psm 6',  # ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆãªã—
            r'--oem 1 --psm 10 -c tessedit_char_whitelist=0123456789',  # LSTM + å˜ä¸€æ–‡å­—
            r'--oem 1 --psm 8',  # LSTM + ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆãªã—
        ]
        
        best_result = ("", 0.0)
        all_results = []
        
        # å…¨ã¦ã®çµ„ã¿åˆã‚ã›ã‚’è©¦ã™
        for method_name, processed_image in preprocessed_images:
            for config in ocr_configs:
                try:
                    # ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
                    text = pytesseract.image_to_string(processed_image, config=config).strip()
                    
                    # æ•°å­—ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
                    import re
                    
                    # ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚‚è¨˜éŒ²ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    if text and len(text.strip()) > 0:
                        logger.debug(f"{region_name} [{method_name}]: ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ='{text.strip()}'")
                    
                    digits_only = re.sub(r'[^0-9]', '', text)
                    
                    if digits_only:
                        # ä¿¡é ¼åº¦å–å¾—
                        try:
                            data = pytesseract.image_to_data(processed_image, config=config, output_type=pytesseract.Output.DICT)
                            confidences = [conf for conf in data['conf'] if conf > 0]
                            avg_conf = sum(confidences) / len(confidences) if confidences else 0
                        except:
                            avg_conf = 50  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¿¡é ¼åº¦
                        
                        result = (digits_only, avg_conf / 100.0)
                        all_results.append((method_name, config, result))
                        
                        if avg_conf > best_result[1] * 100:
                            best_result = result
                            logger.info(f"{region_name}: '{digits_only}' (ä¿¡é ¼åº¦: {avg_conf:.1f}) [{method_name}]")
                    
                    # æ•°å­—ãŒãªãã¦ã‚‚ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹å ´åˆã¯è¨˜éŒ²
                    elif text and len(text.strip()) > 0:
                        logger.debug(f"{region_name} [{method_name}]: æ•°å­—ä»¥å¤–='{text.strip()}'")
                
                except Exception as e:
                    logger.debug(f"{region_name} [{method_name}] OCRã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        # è¨˜å…¥è€…ç•ªå·ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆ"No. 3" -> "3"ï¼‰
        if region_name == "è¨˜å…¥è€…ç•ªå·" and not best_result[0]:
            # "No."ã‚’é™¤ã„ãŸå‡¦ç†ã‚’è©¦ã™
            for method_name, processed_image in preprocessed_images:
                try:
                    # ã‚ˆã‚Šå¯›å®¹ãªè¨­å®š
                    config = r'--oem 3 --psm 6'
                    text = pytesseract.image_to_string(processed_image, config=config).strip()
                    
                    # "No. X" ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
                    import re
                    match = re.search(r'(?:No\.?\s*)?(\d+)', text, re.IGNORECASE)
                    if match:
                        digit = match.group(1)
                        best_result = (digit, 0.8)  # é«˜ä¿¡é ¼åº¦ã‚’è¨­å®š
                        logger.info(f"{region_name}: '{digit}' (ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ) [{method_name}]")
                        break
                
                except Exception:
                    continue
        
        logger.info(f"{region_name} æœ€çµ‚çµæœ: '{best_result[0]}' (ä¿¡é ¼åº¦: {best_result[1]:.2f}) [Tesseract]")
        return best_result
    
    def detect_score_and_comment_boxes(self, gray: np.ndarray, debug=False) -> Tuple[List[Tuple[int, int, int, int]], List[Tuple[int, int, int, int]]]:
        """page_split.pyã®ç‚¹æ•°ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯"""
        H, W = gray.shape
        roi_x0 = int(W * 0.6)  # å³ 40% ã ã‘è¦‹ã‚‹
        roi = gray[:, roi_x0:]
        
        # äºŒå€¤åŒ– & å‰å‡¦ç†
        th = cv2.adaptiveThreshold(roi, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 6)
        th = cv2.morphologyEx(th, cv2.MORPH_OPEN, np.ones((3, 3), np.uint8), 1)
        th = cv2.dilate(th, np.ones((3, 3), np.uint8), 1)
        
        # è¼ªéƒ­
        cnts, _ = cv2.findContours(th, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        
        score_cand, cmt_cand = [], []
        for c in cnts:
            x, y, w, h = cv2.boundingRect(c)
            if w < 20 or h < 30:  # å°ã•ã™ãã‚‹ã‚‚ã®é™¤å¤–
                continue
            
            ratio = w / h
            area = w * h
            if area < 600:  #20Ã—30
                continue
            
            # (x,y) ã‚’ãƒ•ãƒ«ç”»åƒåº§æ¨™ç³»ã«ç›´ã™
            X = x + roi_x0
            
            # ç‚¹æ•°å€™è£œ: æ­£æ–¹å½¢ã€œã‚„ã‚„ç¸¦é•·
            if 0.8 < ratio < 1.3:
                score_cand.append((X, y, w, h))
            # ã‚³ãƒ¡ãƒ³ãƒˆå€™è£œ: æ¨ªé•·
            elif ratio > 4.5:
                cmt_cand.append((X, y, w, h))
        
        if not score_cand or not cmt_cand:
            raise RuntimeError("ç‚¹æ•°æ  or ã‚³ãƒ¡ãƒ³ãƒˆæ ãŒæ¤œå‡ºã§ãã¾ã›ã‚“")
        
        if debug:
            dbg_roi = cv2.cvtColor(th, cv2.COLOR_GRAY2BGR)
            for X, Y, w, h in score_cand:
                cv2.rectangle(dbg_roi, (X - roi_x0, Y), (X - roi_x0 + w, Y + h), (0, 255, 255), 1)
            cv2.imwrite(str(self.debug_dir / "dbg_score_candidates.jpg"), dbg_roi)
        
        # å³ç«¯åˆ—ã ã‘æ®‹ã™
        def keep_rightmost(boxes, tol=25):
            # tol: 25px ä»¥å†…ã®ã‚‚ã®ã‚’å³ç«¯ã¨ã¿ãªã™
            max_x = max(b[0] for b in boxes)
            return [b for b in boxes if abs(b[0] - max_x) < tol]
        
        score_cand = keep_rightmost(score_cand, 40)  # 40px ä»¥å†…
        cmt_cand = keep_rightmost(cmt_cand)
        
        # ç‚¹æ•°æ å€™è£œã®å¹³å‡ã®å¹…ã‚’è¨ˆç®—
        if len(score_cand) > 0:
            #avg_w = sum(b[2] for b in score_cand) / len(score_cand)
            # å¹…ãŒå¹³å‡ã® 0.8ã€œ1.2 å€ã®ã‚‚ã®ã ã‘æ®‹ã™
            #score_cand = [b for b in score_cand if 0.8 * avg_w < b[2] < 1.2 * avg_w]
            widths = np.array([b[2] for b in score_cand])
            q1, q3   = np.percentile(widths, [25, 75])
            iqr      = q3 - q1
            lo, hi   = q1 - 1.5*iqr, q3 + 1.5*iqr      # å¤–ã‚Œå€¤ã—ãã„
            score_cand = [b for b in score_cand if lo < b[2] < hi]

        # y æ˜‡é †ã« 12 å€‹ãšã¤ãã‚ãˆã‚‹
        score_cand = sorted(score_cand, key=lambda b: b[1])[:12]
        cmt_cand = sorted(cmt_cand, key=lambda b: b[1])[:12]
        
        # margin ã‚’å†…å´ã¸å…¥ã‚Œã¦ (x1,y1,x2,y2) ã«å¤‰æ›
        def to_box(lst, margin=4):
            return [(x+margin, y+margin, x+w-margin, y+h-margin) for (x,y,w,h) in lst]
        
        score_boxes = to_box(score_cand)
        cmt_boxes = to_box(cmt_cand)
        
        # ãƒ‡ãƒãƒƒã‚°æç”»
        if debug:
            dbg_img = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
            for x1,y1,x2,y2 in score_boxes:
                cv2.rectangle(dbg_img, (x1,y1), (x2,y2), (0,0,255), 2)  # èµ¤=ç‚¹æ•°
            for x1,y1,x2,y2 in cmt_boxes:
                cv2.rectangle(dbg_img, (x1,y1), (x2,y2), (255,0,0), 2)  # é’=ã‚³ãƒ¡ãƒ³ãƒˆ
            cv2.imwrite(str(self.debug_dir / "dbg_score_comment_boxes.jpg"), dbg_img)
        
        if len(score_boxes) != 12 or len(cmt_boxes) != 12:
            raise RuntimeError(f"æ¤œå‡ºæ•°  score:{len(score_boxes)}  cmt:{len(cmt_boxes)}")
        
        return score_boxes, cmt_boxes
    
    def extract_writer_id_region(self, corrected_image: np.ndarray) -> np.ndarray:
        """page_split.pyãƒ™ãƒ¼ã‚¹ã®è¨˜å…¥è€…ç•ªå·æŠ½å‡º"""
        height, width = corrected_image.shape[:2]
        # å›ºå®šæ¯”ç‡ã§è¨˜å…¥è€…ç•ªå·é ˜åŸŸã‚’æŠ½å‡º
        x, y, w, h = (0.73, 0.03, 0.22, 0.05)
        x1, y1 = int(width * x), int(height * y)
        x2, y2 = int(width * (x + w)), int(height * (y + h))
        return corrected_image[y1:y2, x1:x2]
    
    def extract_number_regions_from_original(self, original_image: np.ndarray, debug=False) -> Dict:
        """å‹•çš„ç‚¹æ•°ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ æŠ½å‡º + è¨˜å…¥è€…ç•ªå·æŠ½å‡º"""
        gray = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY) if len(original_image.shape) == 3 else original_image
        
        # é€è¦–å¤‰æ›é©ç”¨
        corrected = self.correct_perspective(original_image, debug)
        corrected_gray = cv2.cvtColor(corrected, cv2.COLOR_BGR2GRAY) if len(corrected.shape) == 3 else corrected
        
        number_results = {
            "writer_number": {},
            "evaluations": {},
            "comments": {}
        }
        
        try:
            # è¨˜å…¥è€…ç•ªå·æŠ½å‡º
            writer_region = self.extract_writer_id_region(corrected_gray)
            cv2.imwrite(str(self.debug_dir / "improved_writer_id.jpg"), writer_region)
            text, confidence = self.perform_enhanced_number_ocr(writer_region, "è¨˜å…¥è€…ç•ªå·")
            number_results["writer_number"] = {
                "text": text,
                "confidence": confidence,
                "bbox": "dynamic"
            }
            
            # ç‚¹æ•°ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆæ æ¤œå‡º
            score_boxes, cmt_boxes = self.detect_score_and_comment_boxes(corrected_gray, debug)
            
            # ç‚¹æ•°æ å‡¦ç†
            eval_names = ["ç™½è©•ä¾¡1", "é»’è©•ä¾¡1", "å ´è©•ä¾¡1", "å½¢è©•ä¾¡1",
                         "ç™½è©•ä¾¡2", "é»’è©•ä¾¡2", "å ´è©•ä¾¡2", "å½¢è©•ä¾¡2",
                         "ç™½è©•ä¾¡3", "é»’è©•ä¾¡3", "å ´è©•ä¾¡3", "å½¢è©•ä¾¡3"]
            
            for idx, (x1, y1, x2, y2) in enumerate(score_boxes):
                if idx < len(eval_names):
                    name = eval_names[idx]
                    region_image = corrected_gray[y1:y2, x1:x2]
                    score_file = self.debug_dir / f"improved_score_{idx+1}.jpg"
                    cv2.imwrite(str(score_file), region_image)
                    text, confidence = self.perform_enhanced_number_ocr(region_image, name)
                    number_results["evaluations"][name] = {
                        "text": text,
                        "confidence": confidence,
                        "bbox": (x1, y1, x2, y2)
                    }
            
            # ã‚³ãƒ¡ãƒ³ãƒˆæ å‡¦ç†
            comment_names = ["ç™½ã‚³ãƒ¡ãƒ³ãƒˆ1", "é»’ã‚³ãƒ¡ãƒ³ãƒˆ1", "å ´ã‚³ãƒ¡ãƒ³ãƒˆ1", "å½¢ã‚³ãƒ¡ãƒ³ãƒˆ1",
                           "ç™½ã‚³ãƒ¡ãƒ³ãƒˆ2", "é»’ã‚³ãƒ¡ãƒ³ãƒˆ2", "å ´ã‚³ãƒ¡ãƒ³ãƒˆ2", "å½¢ã‚³ãƒ¡ãƒ³ãƒˆ2",
                           "ç™½ã‚³ãƒ¡ãƒ³ãƒˆ3", "é»’ã‚³ãƒ¡ãƒ³ãƒˆ3", "å ´ã‚³ãƒ¡ãƒ³ãƒˆ3", "å½¢ã‚³ãƒ¡ãƒ³ãƒˆ3"]
            
            for idx, (x1, y1, x2, y2) in enumerate(cmt_boxes):
                if idx < len(comment_names):
                    name = comment_names[idx]
                    region_image = corrected_gray[y1:y2, x1:x2]
                    comment_file = self.debug_dir / f"improved_comment_{idx+1}.jpg"
                    cv2.imwrite(str(comment_file), region_image)
                    # ã‚³ãƒ¡ãƒ³ãƒˆã¯OCRã—ãªã„ã§ä¿å­˜ã®ã¿
                    number_results["comments"][name] = {
                        "bbox": (x1, y1, x2, y2),
                        "image_saved": str(comment_file)
                    }
        
        except Exception as e:
            logger.warning(f"å‹•çš„æ¤œå‡ºå¤±æ•—: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç©ºã®çµæœã‚’è¿”ã™
        
        return number_results
    
    def process_form(self, image_path: str, debug=False) -> Dict:
        """æ”¹è‰¯ç‰ˆè¨˜å…¥ç”¨ç´™å‡¦ç†ï¼ˆpage_split.pyãƒ­ã‚¸ãƒƒã‚¯çµ±åˆç‰ˆï¼‰"""
        logger.info(f"æ”¹è‰¯ç‰ˆå‡¦ç†é–‹å§‹: {image_path}")
        
        # ç”»åƒèª­ã¿è¾¼ã¿
        original_image = cv2.imread(image_path)
        if original_image is None:
            raise FileNotFoundError(f"ç”»åƒãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“: {image_path}")
        
        logger.info(f"å…ƒç”»åƒã‚µã‚¤ã‚º: {original_image.shape}")
        
        # é€è¦–å¤‰æ›é©ç”¨ï¼ˆA4ç”»åƒå…¨ä½“ç…§æ˜è£œæ­£ã‚’å«ã‚€ï¼‰
        corrected_image = self.correct_perspective(original_image, debug=True)
        cv2.imwrite(str(self.debug_dir / "improved_corrected.jpg"), corrected_image)
        
        # æ–‡å­—é ˜åŸŸæŠ½å‡ºï¼ˆå‹•çš„æ¤œå‡ºï¼‰
        character_images = self.extract_character_regions(corrected_image, debug)
        
        # æ•°å­—ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆé ˜åŸŸæŠ½å‡ºã¨OCRï¼ˆå‹•çš„æ¤œå‡ºï¼‰
        number_results = self.extract_number_regions_from_original(original_image, debug)
        
        # çµæœçµ±åˆ
        results = {
            "correction_applied": True,
            "character_results": character_images,  # Geminièªè­˜çµæœå«ã‚€
            "writer_number": number_results["writer_number"],
            "evaluations": number_results["evaluations"],
            "comments": number_results["comments"],
            "gemini_enabled": self.use_gemini,
            "pytorch_enabled": self.use_pytorch
        }
        
        logger.info("æ”¹è‰¯ç‰ˆå‡¦ç†å®Œäº†")
        return results


def main():
    """æ”¹è‰¯ç‰ˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆpage_split.pyçµ±åˆ + Geminièªè­˜ï¼‰"""
    
    logging.basicConfig(level=logging.INFO)
    
    # Geminiåˆ©ç”¨å¯å¦ã‚’ãƒã‚§ãƒƒã‚¯
    use_gemini = os.getenv('GEMINI_API_KEY') is not None
    if use_gemini:
        print("ğŸš€ Gemini APIæœ‰åŠ¹åŒ–: æ–‡å­—èªè­˜ã«Geminiã‚’ä½¿ç”¨")
    else:
        print("âš ï¸ Gemini APIç„¡åŠ¹: .envãƒ•ã‚¡ã‚¤ãƒ«ã«GEMINI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„")
    
    # PyTorchãƒ¢ãƒ‡ãƒ«åˆ©ç”¨å¯å¦ã‚’ãƒã‚§ãƒƒã‚¯
    pytorch_model_path = "/workspace/data/digit_model.pt"
    if os.path.exists(pytorch_model_path):
        print("ğŸ§  PyTorchæ•°å­—èªè­˜ãƒ¢ãƒ‡ãƒ«æœ‰åŠ¹åŒ–: é«˜ç²¾åº¦æ•°å­—èªè­˜ã‚’ä½¿ç”¨")
    else:
        print("âš ï¸ PyTorchãƒ¢ãƒ‡ãƒ«ç„¡åŠ¹: digit_model.ptãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    processor = ImprovedOCRProcessor(use_gemini=use_gemini)
    
    try:
        #results = processor.process_form("docs/è¨˜å…¥sample.JPG", debug=True)
        results = processor.process_form("debug/original_input.jpg", debug=True)

        print("\n=== æ”¹è‰¯ç‰ˆå‡¦ç†çµæœï¼ˆpage_split.pyçµ±åˆ + Geminièªè­˜ï¼‰ ===")
        print(f"æ­ªã¿è£œæ­£: {'é©ç”¨' if results['correction_applied'] else 'æœªé©ç”¨'}")
        
        print("\n[æ–‡å­—ç”»åƒæŠ½å‡ºãƒ»èªè­˜ï¼ˆå‹•çš„æ¤œå‡º + Geminiï¼‰]")
        for char_name, char_data in results["character_results"].items():
            print(f"  {char_name}: debug/improved_char_{char_name}.jpg ä¿å­˜æ¸ˆã¿")
            
            # Geminièªè­˜çµæœã‚’è¡¨ç¤º
            gemini_result = char_data.get("gemini_recognition", {})
            if gemini_result.get("character"):
                char = gemini_result["character"]
                conf = gemini_result["confidence"]
                status = "âœ“" if conf > 0.5 else "âš "
                print(f"    â†’ Geminièªè­˜: '{char}' (ä¿¡é ¼åº¦: {conf:.2f}) {status}")
                
                # ä»£æ›¿å€™è£œãŒã‚ã‚Œã°è¡¨ç¤º
                alternatives = gemini_result.get("alternatives", [])
                if alternatives:
                    print(f"    â†’ ä»£æ›¿å€™è£œ: {', '.join(alternatives[:3])}")
            else:
                method = gemini_result.get("method", "unknown")
                print(f"    â†’ Geminièªè­˜: å¤±æ•— ({method})")
        
        print("\n[è¨˜å…¥è€…ç•ªå·ï¼ˆå‹•çš„æ¤œå‡ºï¼‰]")
        writer_data = results["writer_number"]
        if writer_data:
            status = "âœ“" if writer_data["text"] else "âœ—"
            print(f"  è¨˜å…¥è€…ç•ªå·: '{writer_data['text']}' (ä¿¡é ¼åº¦: {writer_data['confidence']:.2f}) {status}")
        
        print("\n[è©•ä¾¡æ•°å­—ï¼ˆå‹•çš„æ¤œå‡ºï¼‰]")
        for eval_name, eval_data in results["evaluations"].items():
            status = "âœ“" if eval_data["text"] else "âœ—"
            print(f"  {eval_name}: '{eval_data['text']}' (ä¿¡é ¼åº¦: {eval_data['confidence']:.2f}) {status}")
        
        print("\n[ã‚³ãƒ¡ãƒ³ãƒˆæ ï¼ˆå‹•çš„æ¤œå‡ºï¼‰]")
        for comment_name, comment_data in results["comments"].items():
            saved_path = Path(comment_data['image_saved'])
            print(f"  {comment_name}: {saved_path} ä¿å­˜æ¸ˆã¿")
        
        print(f"\næ”¹è‰¯ç‰ˆå‡¦ç†å®Œäº†ï¼ï¼ˆå‹•çš„æ¤œå‡º + Geminiæ–‡å­—èªè­˜ + PyTorchæ•°å­—èªè­˜çµ±åˆï¼‰")
        print(f"Gemini APIä½¿ç”¨: {'âœ…' if results.get('gemini_enabled') else 'âŒ'}")
        print(f"PyTorchæ•°å­—èªè­˜ä½¿ç”¨: {'âœ…' if results.get('pytorch_enabled') else 'âŒ'}")
        
    except Exception as e:
        logger.error(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()